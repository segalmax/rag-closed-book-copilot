# RAG Application â€” Step 2: Knowledge Base

## Data Source

System design interview prep articles scraped from [hellointerview.com](https://www.hellointerview.com/learn/system-design).  
26 articles covering core concepts, key technologies (Redis, Kafka, PostgreSQL, etc.), and advanced topics (vector databases, etc.).  
Each article is stored as a markdown file with YAML frontmatter (`url`, `title`, `free`, `scraped_at`) under `kb/raw/`.

---

## Chunking Strategy

Two-stage split using LangChain:

1. **Section split** â€” `MarkdownHeaderTextSplitter` splits each article on H1/H2/H3 headers. Each section carries `section_title` and `section_path` metadata.
2. **Chunk split** â€” `RecursiveCharacterTextSplitter` (tiktoken `cl100k_base`, 700 tokens, 120 overlap) splits each section into token-sized chunks. All non-empty chunks are kept.

Result: **1,108 chunks** across 1,084 sections from 26 documents.  
Sections â‰ˆ chunks because most sections are short enough to fit within the 700-token limit â€” each section produces exactly one chunk. Only ~24 sections are long enough to split further. This is expected for a well-structured study KB.

```mermaid
%%{ init: { 'theme': 'base' } }%%
flowchart TD
    classDef file      fill:#dbeafe,stroke:#3b82f6,color:#1e3a5f
    classDef process   fill:#dcfce7,stroke:#16a34a,color:#14532d
    classDef artifact  fill:#ede9fe,stroke:#7c3aed,color:#2e1065
    classDef splitter  fill:#fef9c3,stroke:#ca8a04,color:#713f12
    classDef embed     fill:#fee2e2,stroke:#dc2626,color:#7f1d1d
    classDef user      fill:#bfdbfe,stroke:#2563eb,color:#1e3a5f
    classDef retrieval fill:#fef3c7,stroke:#d97706,color:#78350f
    classDef llm       fill:#fce7f3,stroke:#db2777,color:#831843
    classDef metric    fill:#ecfdf5,stroke:#059669,color:#064e3b

    subgraph INGEST["ğŸ—ï¸  INGEST + EMBED â€” runs once"]
        direction LR
        MD["ğŸ“„ kb/raw/*.md<br/>YAML frontmatter + body"]:::file
        P["â‘¡ preprocess_kb.py<br/>extract title Â· normalize text"]:::process
        S["â‘¢ MarkdownHeaderTextSplitter<br/>split on H1/H2/H3<br/>â†’ section_title metadata"]:::splitter
        C["â‘£ RecursiveCharacterTextSplitter<br/>700 tok Â· 120 overlap<br/>cl100k_base tokenizer"]:::splitter
        E["â‘¤ embed.py Â· mxbai-embed-large-v1<br/>prefix: Title > Section: text<br/>â†’ 1024-dim Â· L2-normalize"]:::embed

        MD -->|"â‘  scrape"| P --> S --> C --> E
    end

    IDX[("ğŸ’¾ kb/index.faiss<br/>1108 Ã— 1024 vectors")]:::artifact
    META[("ğŸ’¾ kb/index_meta.json<br/>chunk text + metadata")]:::artifact

    E -->|"index.add()"| IDX
    C -->|"save chunks"| META

    subgraph QUERY["ğŸ’¬  QUERY PIPELINE â€” per user request"]
        direction LR
        USR(["ğŸ‘¤ User Â· Streamlit UI"]):::user
        QE["â‘¡ Query Encoder<br/>search prefix + mxbai-embed-large-v1<br/>â†’ 1024-dim Â· L2-normalize"]:::embed
        VS["â‘¢ Dense Retrieval<br/>IndexFlatIP.search(q, k)<br/>â†’ top-k indices + scores"]:::retrieval
        CTX["â‘£ Context Builder<br/>fetch chunk text + title by index"]:::retrieval
        LLM["â‘¤ GPT-4o<br/>answer ONLY from context<br/>stream: True"]:::llm
        ANS["â‘¥ Streamlit renders<br/>answer token-by-token"]:::user

        USR -->|"â‘  types question"| QE
        QE -->|"query vector"| VS
        VS -->|"top-k indices"| CTX
        CTX -->|"context string"| LLM
        LLM -->|"stream tokens"| ANS
    end

    VS -->|"lookup"| IDX
    CTX -->|"lookup"| META

    subgraph EVAL["ğŸ§ª  EVAL PIPELINE â€” offline quality measurement"]
        direction LR
        TESTS["ğŸ“‹ tests.jsonl Â· 23 questions<br/>categories: direct_fact Â· temporal<br/>comparative Â· numerical<br/>relationship Â· spanning Â· holistic"]:::file
        RETRIEVE["rag.retrieve(question, k=5)<br/>â†’ top-k chunks"]:::retrieval
        RMETRICS["Retrieval Metrics<br/>MRR â€” mean reciprocal rank of keyword hits<br/>NDCG â€” rank-weighted relevance score<br/>keyword coverage â€” % keywords in top-k"]:::metric
        ROUT["ğŸ’¾ last_run_retrieval.json"]:::artifact
        GEN["rag.generate_answer(model=gpt-4o)<br/>stream=False Â· full response"]:::llm
        JUDGE["GPT-4o judge<br/>AnswerEval via Pydantic structured output<br/>prompt: question + generated + reference"]:::llm
        AMETRICS["Answer Metrics  (1â€“5 each)<br/>accuracy â€” factual correctness<br/>completeness â€” all aspects covered<br/>relevance â€” directness to question"]:::metric
        AOUT["ğŸ’¾ last_run_answer.json"]:::artifact

        TESTS -->|"per question"| RETRIEVE
        RETRIEVE --> RMETRICS --> ROUT
        RETRIEVE -->|"top-k chunks"| GEN
        GEN --> JUDGE --> AMETRICS --> AOUT
    end

    RETRIEVE -->|"lookup"| IDX
    RETRIEVE -->|"lookup"| META

    subgraph LEGEND["Legend"]
        direction LR
        L1["ğŸ“„ Raw file / input"]:::file
        L2["âš™ï¸ Processing step"]:::process
        L3["âœ‚ï¸ Text splitting"]:::splitter
        L4["ğŸ”¢ Embedding / encoding"]:::embed
        L5["ğŸ” Dense retrieval"]:::retrieval
        L6["ğŸ¤– LLM call"]:::llm
        L7["ğŸ“Š Evaluation metric"]:::metric
        L8["ğŸ‘¤ User / UI"]:::user
        L9[("ğŸ’¾ Persistent storage")]:::artifact
    end
```

---

## Embedding Model

- **Model**: `mixedbread-ai/mxbai-embed-large-v1` (Hugging Face, via `sentence-transformers`)
- **Dimensions**: 1024
- **Contextual prefix**: each chunk is prepended with `Title > Section: ` before embedding, grounding it semantically in its document context
- **Query prefix**: `Represent this sentence for searching relevant passages: {query}`
- **Normalization**: L2-normalized so cosine similarity = inner product

---

## FAISS Indexing

- **Index type**: `IndexFlatIP` â€” exact inner-product search (= cosine similarity after L2 normalization)
- **Pipeline**: `chunks.jsonl` â†’ encode â†’ cast to `float32` â†’ L2-normalize â†’ `index.add()`
- **Files**: `kb/index.faiss` (vectors), `kb/index_meta.json` (full chunk metadata list, indexed by position)
- **Why `IndexFlatIP` and not HNSW**: at 1,108 chunks, brute-force O(n) search is instant. HNSW is appropriate for 100k+ vectors; using it here would be premature complexity with no practical benefit.

---

## Semantic Retrieval Validation

Retrieval quality is measured via `evaluation/eval.py` against 23 hand-written test questions across 7 categories (`direct_fact`, `temporal`, `comparative`, `numerical`, `relationship`, `spanning`, `holistic`).

Metrics per question:
- **MRR** â€” rank of first chunk containing an expected keyword
- **NDCG** â€” rank-weighted keyword coverage
- **Keyword coverage** â€” % of expected keywords found in top-k chunks

The LLM is instructed to answer **only from retrieved context** (`"Answer based ONLY on the provided context"`), and to explicitly refuse if the answer is not present â€” reducing hallucination to near zero for out-of-KB queries.

Edge cases handled: empty query (text input guard), no relevant results (system prompt instructs refusal), API errors (caught and displayed in UI).

---

## Reflection

**What worked well:**
- Contextual chunking prefix (`Title > Section: text`) measurably improves retrieval by grounding short chunks in their document context
- `IndexFlatIP` with L2-normalized embeddings gives exact cosine similarity with zero approximation error â€” correct for this KB size
- LLM-as-judge evaluation (GPT-4o scoring accuracy/completeness/relevance) gives interpretable quality signal beyond keyword matching

**What could be improved:**
- Trafilatura converts HTML tables and code blocks to `##` headers â€” affects 6% of chunks (67/1,108), mostly in `zookeeper.md` and `postgresql.md`. Switching to `markdownify` would fix this.
- 120-token average chunk size is small; merging very short sections into neighbors before chunking would reduce noise in the index.
